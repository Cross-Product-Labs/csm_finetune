{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize Expresso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "ds = load_from_disk(\"../datasets/encoded_expresso\")\n",
    "ds = ds.with_format('torch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going to drop everything else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "supported_styles = [\"confused\", \"enunciated\", \"happy\", \"laughing\", \"default\", \"sad\", \"whisper\", \"emphasis\"]\n",
    "ds = ds.filter(lambda r: r[\"style\"] in supported_styles, num_proc=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add control tokens to model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "init_folder = \"../inits/csm-1b-expresso\"\n",
    "os.makedirs(init_folder, exist_ok=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"unsloth/Llama-3.2-1B\")\n",
    "n_added_tokens = tokenizer.add_special_tokens({\"additional_special_tokens\": [\n",
    "    \"<|confused|>\",\n",
    "    \"<|enunciated|>\",\n",
    "    \"<|happy|>\",\n",
    "    \"<|laughing|>\",\n",
    "    \"<|default|>\",\n",
    "    \"<|sad|>\",\n",
    "    \"<|whisper|>\",\n",
    "    \"<|emphasis|>\",\n",
    "    ]\n",
    "})\n",
    "\n",
    "tokenizer.save_pretrained(init_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch the checkpoint\n",
    "from huggingface_hub import hf_hub_download\n",
    "from safetensors.torch import load_file, save_file\n",
    "import torch\n",
    "\n",
    "model_path = hf_hub_download(repo_id=\"sesame/csm-1b\", filename=\"model.safetensors\")\n",
    "\n",
    "state_dict = load_file(model_path, device=\"cpu\")\n",
    "\n",
    "mean_embedding = state_dict[\"text_embeddings.weight\"].mean(dim=0, keepdim=True)\n",
    "expanded_embedding = mean_embedding.expand(n_added_tokens, -1)\n",
    "state_dict[\"text_embeddings.weight\"] = torch.cat([state_dict[\"text_embeddings.weight\"], expanded_embedding], dim=0)\n",
    "\n",
    "save_file(state_dict, f\"{init_folder}/model.safetensors\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "config = hf_hub_download(repo_id=\"sesame/csm-1b\", filename=\"config.json\")\n",
    "with open(config, 'r') as f:\n",
    "    config_json = json.load(f)\n",
    "\n",
    "config_json[\"text_vocab_size\"] += n_added_tokens\n",
    "with open(f\"{init_folder}/config.json\", 'w') as f:\n",
    "    json.dump(config_json, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize to CSM format\n",
    "\n",
    "Now let's load our new tokenizer back again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modeling.utils import PromptEncoder\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(init_folder)\n",
    "prompt_encoder = PromptEncoder(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we prepare the inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def tokenize_row(row: dict):\n",
    "    # Abuse turn ID as voice prompt, this is just for testing\n",
    "    text_tokens, text_masks = prompt_encoder._tokenize_text_segment(\n",
    "        f'<|{row[\"style\"]}|>{row[\"text\"]}', int(row[\"speaker_id\"][-1]) - 1\n",
    "    )\n",
    "    audio_tokens, audio_masks = prompt_encoder._tokenize_audio(row['codes'])\n",
    "\n",
    "    return {\n",
    "        \"ground_truth\": torch.cat([text_tokens, audio_tokens], dim=0), \n",
    "        \"ground_truth_masks\": torch.cat([text_masks, audio_masks], dim=0),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "orig_colnames = ds.column_names\n",
    "ds = ds.map(tokenize_row, num_proc=12, remove_columns=orig_colnames)\n",
    "\n",
    "ds = DatasetDict({\n",
    "    \"train\": ds\n",
    "})\n",
    "ds.save_to_disk(\"../datasets/tokenized_expresso\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
