{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize Expresso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "ds = load_from_disk(\"../datasets/encoded_expresso\")\n",
    "ds = ds.with_format('torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text', 'speaker_id', 'style', 'id', 'codes']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_76956/2534450039.py:4: FutureWarning: pandas.value_counts is deprecated and will be removed in a future version. Use pd.Series(obj).value_counts() instead.\n",
      "  pd.value_counts(pd.Series(ds['style']))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "confused      1520\n",
       "enunciated    1520\n",
       "happy         1520\n",
       "laughing      1520\n",
       "default       1519\n",
       "sad           1519\n",
       "whisper       1518\n",
       "emphasis       800\n",
       "essentials     160\n",
       "singing         10\n",
       "longform         8\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(ds.column_names)\n",
    "pd.value_counts(pd.Series(ds['style']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going to drop everything else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "supported_styles = [\"confused\", \"enunciated\", \"happy\", \"laughing\", \"default\", \"sad\", \"whisper\", \"emphasis\"]\n",
    "ds = ds.filter(lambda r: r[\"style\"] in supported_styles, num_proc=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create control tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../inits/csm-1b-expresso/tokenizer_config.json',\n",
       " '../inits/csm-1b-expresso/special_tokens_map.json',\n",
       " '../inits/csm-1b-expresso/tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "init_folder = \"../inits/csm-1b-expresso\"\n",
    "os.makedirs(init_folder, exist_ok=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"unsloth/Llama-3.2-1B\")\n",
    "n_added_tokens = tokenizer.add_special_tokens({\"additional_special_tokens\": [\n",
    "    \"<|confused|>\",\n",
    "    \"<|enunciated|>\",\n",
    "    \"<|happy|>\",\n",
    "    \"<|laughing|>\",\n",
    "    \"<|default|>\",\n",
    "    \"<|sad|>\",\n",
    "    \"<|whisper|>\",\n",
    "    \"<|emphasis|>\",\n",
    "    ]\n",
    "})\n",
    "\n",
    "tokenizer.save_pretrained(init_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch the checkpoint\n",
    "from huggingface_hub import hf_hub_download\n",
    "from safetensors.torch import load_file, save_file\n",
    "import torch\n",
    "\n",
    "model_path = hf_hub_download(repo_id=\"sesame/csm-1b\", filename=\"model.safetensors\")\n",
    "\n",
    "state_dict = load_file(model_path, device=\"cpu\")\n",
    "\n",
    "mean_embedding = state_dict[\"text_embeddings.weight\"].mean(dim=0, keepdim=True)\n",
    "expanded_embedding = mean_embedding.expand(n_added_tokens, -1)\n",
    "state_dict[\"text_embeddings.weight\"] = torch.cat([state_dict[\"text_embeddings.weight\"], expanded_embedding], dim=0)\n",
    "\n",
    "save_file(state_dict, f\"{init_folder}/model.safetensors\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "config = hf_hub_download(repo_id=\"sesame/csm-1b\", filename=\"config.json\")\n",
    "with open(config, 'r') as f:\n",
    "    config_json = json.load(f)\n",
    "\n",
    "config_json[\"text_vocab_size\"] += n_added_tokens\n",
    "with open(f\"{init_folder}/config.json\", 'w') as f:\n",
    "    json.dump(config_json, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize to CSM format\n",
    "\n",
    "Now let's load our new tokenizer back again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128000, 58, 15, 60, 128256, 1985]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|>[0]<|confused|>test'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(init_folder)\n",
    "encoded = tokenizer.encode(\"[0]<|confused|>test\")\n",
    "print(encoded)\n",
    "tokenizer.decode(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modeling.utils import PromptEncoder\n",
    "\n",
    "prompt_encoder = PromptEncoder(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we prepare the inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def tokenize_row(row: dict):\n",
    "    text_tokens, text_masks = prompt_encoder._tokenize_text_segment(\n",
    "        f'<|{row[\"style\"]}|>{row[\"text\"]}', 0\n",
    "    )\n",
    "    audio_tokens, audio_masks = prompt_encoder._tokenize_audio(row['codes'])\n",
    "\n",
    "    return {\n",
    "        \"ground_truth\": torch.cat([text_tokens, audio_tokens], dim=0), \n",
    "        \"ground_truth_masks\": torch.cat([text_masks, audio_masks], dim=0),\n",
    "    }\n",
    "\n",
    "# TODO speed this up and/or move it to the collate fn: for libritts it doesn't really matter\n",
    "# ds = ds.map(get_targets, remove_columns=orig_colnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (0/1 shards):   0%|          | 0/11436 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 11436/11436 [00:00<00:00, 217993.86 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "orig_colnames = ds.column_names\n",
    "ds = ds.map(tokenize_row, num_proc=12, remove_columns=orig_colnames)\n",
    "\n",
    "ds = DatasetDict({\n",
    "    \"train\": ds\n",
    "})\n",
    "ds.save_to_disk(\"../datasets/tokenized_expresso\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([51, 33])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_row = ds[0]['ground_truth']\n",
    "example_row.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|>[0]<|confused|>Why are you beating up my jukebox?!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_encoder._text_tokenizer.decode(example_row[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,   1049,\n",
       "           1102,   1686,   1258,   1258,   1689,   1528,   1987,    978,    312,\n",
       "           2039,    753,    969,    598,   1084,   1268,    621,   1757,    560,\n",
       "           1734,   1527,   1117,    622,    628,    510,    623,    623,    918,\n",
       "            689,    997,   1069,   1941,    294,    774,    518,   1987,    769,\n",
       "              0],\n",
       "        [128000,     58,     15,   1483,    791,  18266,     60,    358,    342,\n",
       "          28109,    449,    264,   3169,    315,   5895,     13, 128001,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([example_row[:, 0], example_row[:, -1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100, -100, -100, 1049, 1102, 1686, 1258, 1258, 1689, 1528, 1987,\n",
       "         978,  312, 2039,  753,  969,  598, 1084, 1268,  621, 1757,  560, 1734,\n",
       "        1527, 1117,  622,  628,  510,  623,  623,  918,  689,  997, 1069, 1941,\n",
       "         294,  774,  518, 1987,  769,    0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row = ds['train'][0]\n",
    "audio_positions = row['ground_truth_masks'][1:, :-1].all(dim=1)\n",
    "labels = row['ground_truth'][1:, :-1]\n",
    "labels[~audio_positions] = -100\n",
    "labels[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_range = torch.arange(0, 32 * 2051, 2051)\n",
    "official_range = 2051 * torch.arange(32)\n",
    "assert my_range.eq(official_range).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing collation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from moshi.models import loaders\n",
    "\n",
    "ds_dev = ds['dev'].map(tokenize_row, num_proc=12, remove_columns=ds['dev'].column_names)\n",
    "\n",
    "mimi_weight = hf_hub_download(loaders.DEFAULT_REPO, loaders.MIMI_NAME)\n",
    "mimi = loaders.get_mimi(mimi_weight, device=\"cpu\")\n",
    "\n",
    "quantizer = mimi.quantizer.acoustic_quantizer.vq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = ds_dev[:32]\n",
    "\n",
    "B = len(batch[\"ground_truth\"])\n",
    "CODEBOOK_SIZE=32\n",
    "\n",
    "height = CODEBOOK_SIZE + 1\n",
    "max_input_len = max(item.shape[0] - 1 for item in batch[\"ground_truth\"])\n",
    "\n",
    "B = len(batch[\"ground_truth\"])\n",
    "tokens = torch.full((B, max_input_len, height), 0, dtype=torch.long)  # 2=some <PAD>\n",
    "targets = torch.full((B, max_input_len, 256), 0, dtype=torch.float32)\n",
    "\n",
    "pad_mask = torch.ones(B, max_input_len)\n",
    "\n",
    "for i in range(B):\n",
    "    ground_truth = batch[\"ground_truth\"][i]\n",
    "    ground_truth_masks = batch[\"ground_truth_masks\"][i]\n",
    "\n",
    "    seq_len = ground_truth.shape[0] - 1\n",
    "    tokens[i, :seq_len, :] = ground_truth[:-1, :].clone()\n",
    "\n",
    "    label = ground_truth[1:, :]\n",
    "    # full block of zeros for audio codes\n",
    "    codes = label[:, 1:-1].T\n",
    "    final_residuals = quantizer.decode(codes.unsqueeze(-1)).squeeze(-1)\n",
    "    # zero text positions with the mask\n",
    "    mask = ground_truth_masks[1:, :-1].all(dim=1)\n",
    "    final_residuals[~mask] = 0\n",
    "    targets[i, :seq_len, :] = final_residuals.unsqueeze(0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
